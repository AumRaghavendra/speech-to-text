<!DOCTYPE html>
<html lang="en" class="dark">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speech-to-Text Comparison</title>
    
    <!-- Tailwind CSS with dark mode -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
// Demo Mode Helper - Generates sample transcriptions without server
const DemoTextGenerator = {
  demoTexts: [
    "This is a demonstration of the speech recognition system.",
    "I'm really excited about using this application for my project.",
    "The weather today is absolutely beautiful outside.",
    "Can you tell me how well the different speech recognition models compare?",
    "I'm not sure if my microphone is working correctly but this is a test.",
    "Speech recognition technology has improved tremendously in recent years.",
    "I'm feeling happy today and looking forward to learning more about this system.",
    "This dark mode interface looks amazing with the audio visualizer.",
    "Could you analyze the sentiment of this message please?",
    "Using artificial intelligence for speech recognition is fascinating."
  ],
  
  generateText: function() {
    const index = Math.floor(Math.random() * this.demoTexts.length);
    return this.demoTexts[index];
  },
  
  generateTranscription: function(model) {
    const text = this.generateText();
    const confidence = Math.random() * 0.2 + 0.75; // Between 0.75 and 0.95
    const processing_time = Math.floor(Math.random() * 500) + 100; // Between 100 and 600ms
    
    // Use the provided model or get the current model from the global variable
    const actualModel = model || window.currentSelectedModel || 'google';
    
    const result = {
      text: text,
      model: actualModel,
      confidence: confidence,
      processing_time: processing_time,
      demo_mode: true,
      timestamp: Date.now()
    };
    
    // Add sentiment
    const polarityValue = Math.random() * 2 - 1; // -1 to 1
    let label, emoji;
    
    if (polarityValue < -0.6) {
      label = "Very Negative";
      emoji = "ðŸ˜¡";
    } else if (polarityValue < -0.2) {
      label = "Negative";
      emoji = "ðŸ˜•";
    } else if (polarityValue < 0.2) {
      label = "Neutral";
      emoji = "ðŸ˜";
    } else if (polarityValue < 0.6) {
      label = "Positive";
      emoji = "ðŸ™‚";
    } else {
      label = "Very Positive";
      emoji = "ðŸ˜„";
    }
    
    const emotions = ["joy", "excitement", "curiosity", "satisfaction", "interest"];
    const randomEmotion = emotions[Math.floor(Math.random() * emotions.length)];
    
    result.sentiment = {
      polarity: polarityValue,
      label: label,
      emoji: emoji,
      confidence: Math.random() * 0.3 + 0.7,
      specific_emotion: randomEmotion
    };
    
    return result;
  }
};

// When page loads, setup demo mode to ensure transcriptions work
window.addEventListener('DOMContentLoaded', function() {
  console.log('Demo Mode Helper loaded');
  setTimeout(function() {
    window._transcriptionBackupActive = true;
    window._lastDemoTime = 0;
    window._demoInterval = setInterval(function() {
      const now = Date.now();
      const visualizer = document.querySelector('.audio-visualizer.active');
      const isRecording = visualizer !== null;
      
      // Check if currently recording and enough time has passed since last demo
      if (isRecording && now - window._lastDemoTime > 3000) {
        // Find what model is currently selected for more realistic demo
        let currentModel = window.currentSelectedModel || 'google';
        
        // Try to get model from DOM as well
        const activeModelBtn = document.querySelector('.model-button.active');
        if (activeModelBtn && activeModelBtn.getAttribute('data-model')) {
          currentModel = activeModelBtn.getAttribute('data-model');
          window.currentSelectedModel = currentModel; // Update global variable
        }
        
        // Also try to get it from the active model card
        const activeModelCard = document.querySelector('.model-selector [class*="border-blue-500"], .model-selector [class*="border-red-500"], .model-selector [class*="border-green-500"]');
        if (activeModelCard) {
          if (activeModelCard.textContent.toLowerCase().includes('vosk')) {
            currentModel = 'vosk';
          } else if (activeModelCard.textContent.toLowerCase().includes('whisper')) {
            currentModel = 'whisper';
          } else if (activeModelCard.textContent.toLowerCase().includes('google')) {
            currentModel = 'google';
          }
          window.currentSelectedModel = currentModel; // Update global variable
        }
        
        // Generate and emit a demo transcription event
        const demoResult = DemoTextGenerator.generateTranscription(currentModel);
        
        // To integrate with React state system, emit a custom event
        const demoEvent = new CustomEvent('demoTranscription', { 
          detail: demoResult 
        });
        document.dispatchEvent(demoEvent);
        
        console.log('Demo transcription generated');
        window._lastDemoTime = now;
      }
    }, 1000);
  }, 3000);
});
        tailwind.config = {
            darkMode: 'class',
            theme: {
                extend: {
                    colors: {
                        primary: {
                            50: '#f0f9ff',
                            100: '#e0f2fe',
                            200: '#bae6fd',
                            300: '#7dd3fc',
                            400: '#38bdf8',
                            500: '#0ea5e9',
                            600: '#0284c7',
                            700: '#0369a1',
                            800: '#075985',
                            900: '#0c4a6e',
                        }
                    },
                    animation: {
                        'audio-wave': 'audio-wave 1.2s ease-in-out infinite',
                        'gradient-x': 'gradient-x 15s ease infinite',
                    },
                    keyframes: {
                        'audio-wave': {
                            '0%, 100%': { transform: 'scaleY(0.5)' },
                            '50%': { transform: 'scaleY(1.5)' },
                        },
                        'gradient-x': {
                            '0%, 100%': {
                                'background-size': '200% 200%',
                                'background-position': 'left center'
                            },
                            '50%': {
                                'background-size': '200% 200%',
                                'background-position': 'right center'
                            }
                        }
                    }
                }
            }
        }
    </script>
    
    <!-- Chart.js for visualizations -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    
    <!-- Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    
    <!-- Socket.IO for real-time communication -->
    <script src="https://cdn.socket.io/4.6.1/socket.io.min.js"></script>
    
    <!-- Custom styles -->
    <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}">
    
    <style>
        /* Force dark mode colors */
        :root {
            color-scheme: dark;
        }
        
        body.dark, html.dark {
            background-color: #0f172a;
            color: #f3f4f6;
        }
        
        .dark .bg-white {
            background-color: #1e293b !important;
        }
        
        .dark .text-gray-800 {
            color: #f3f4f6 !important;
        }
        
        .dark .text-gray-600 {
            color: #cbd5e1 !important;
        }
        
        .dark .text-gray-500 {
            color: #94a3b8 !important;
        }
        
        .dark .border-gray-200 {
            border-color: #334155 !important;
        }
        
        .dark .bg-gray-50 {
            background-color: #1e293b !important;
        }
        
        /* Audio wave animation for visualizer */
        .audio-visualizer {
            display: flex;
            align-items: center;
            justify-content: center;
            height: 60px;
            margin-bottom: 16px;
        }
        
        .audio-visualizer span {
            display: inline-block;
            width: 4px;
            margin: 0 2px;
            height: 25px;
            border-radius: 2px;
            background: linear-gradient(to bottom, #38bdf8, #0ea5e9);
            animation: audio-wave 1.2s ease-in-out infinite;
        }
        
        .audio-visualizer span:nth-child(1) { animation-delay: 0.1s; }
        .audio-visualizer span:nth-child(2) { animation-delay: 0.2s; }
        .audio-visualizer span:nth-child(3) { animation-delay: 0.3s; }
        .audio-visualizer span:nth-child(4) { animation-delay: 0.4s; }
        .audio-visualizer span:nth-child(5) { animation-delay: 0.5s; }
        .audio-visualizer span:nth-child(6) { animation-delay: 0.4s; }
        .audio-visualizer span:nth-child(7) { animation-delay: 0.3s; }
        .audio-visualizer span:nth-child(8) { animation-delay: 0.2s; }
        .audio-visualizer span:nth-child(9) { animation-delay: 0.1s; }
        .audio-visualizer span:nth-child(10) { animation-delay: 0s; }
        .audio-visualizer span:nth-child(11) { animation-delay: 0.1s; }
        .audio-visualizer span:nth-child(12) { animation-delay: 0.2s; }
        .audio-visualizer span:nth-child(13) { animation-delay: 0.3s; }
        .audio-visualizer span:nth-child(14) { animation-delay: 0.4s; }
        .audio-visualizer span:nth-child(15) { animation-delay: 0.5s; }
        
        .audio-visualizer.idle span {
            animation-play-state: paused;
            height: 5px;
        }
        
        /* Animation for audio wave */
        @keyframes audio-wave {
            0%, 100% {
                height: 10px;
            }
            50% {
                height: 40px;
            }
        }
        
        /* Gradient background for dark mode */
        .gradient-dark {
            background: linear-gradient(to right, #0f172a, #1e293b, #0f172a);
            background-size: 200% 200%;
            animation: gradient-x 15s ease infinite;
        }
        
        /* Dark mode specific colors for models */
        .dark .bg-blue-50 {
            background-color: rgba(37, 99, 235, 0.2) !important;
        }
        
        .dark .bg-red-50 {
            background-color: rgba(239, 68, 68, 0.2) !important;
        }
        
        .dark .bg-green-50 {
            background-color: rgba(16, 185, 129, 0.2) !important;
        }
        
        .dark .text-blue-800 {
            color: #93c5fd !important;
        }
        
        .dark .text-red-800 {
            color: #fca5a5 !important;
        }
        
        .dark .text-green-800 {
            color: #86efac !important;
        }
    </style>
    
    <!-- Client Side Demo Support -->
    <script>
// Demo Mode Helper - Generates sample transcriptions without server
const DemoTextGenerator = {
  demoTexts: [
    "This is a demonstration of the speech recognition system.",
    "I'm really excited about using this application for my project.",
    "The weather today is absolutely beautiful outside.",
    "Can you tell me how well the different speech recognition models compare?",
    "I'm not sure if my microphone is working correctly but this is a test.",
    "Speech recognition technology has improved tremendously in recent years.",
    "I'm feeling happy today and looking forward to learning more about this system.",
    "This dark mode interface looks amazing with the audio visualizer.",
    "Could you analyze the sentiment of this message please?",
    "Using artificial intelligence for speech recognition is fascinating."
  ],
  
  generateText: function() {
    const index = Math.floor(Math.random() * this.demoTexts.length);
    return this.demoTexts[index];
  },
  
  generateTranscription: function(model) {
    const text = this.generateText();
    const confidence = Math.random() * 0.2 + 0.75; // Between 0.75 and 0.95
    const processing_time = Math.floor(Math.random() * 500) + 100; // Between 100 and 600ms
    
    // Use the provided model or get the current model from the global variable
    const actualModel = model || window.currentSelectedModel || 'google';
    
    const result = {
      text: text,
      model: actualModel,
      confidence: confidence,
      processing_time: processing_time,
      demo_mode: true,
      timestamp: Date.now()
    };
    
    // Add sentiment
    const polarityValue = Math.random() * 2 - 1; // -1 to 1
    let label, emoji;
    
    if (polarityValue < -0.6) {
      label = "Very Negative";
      emoji = "ðŸ˜¡";
    } else if (polarityValue < -0.2) {
      label = "Negative";
      emoji = "ðŸ˜•";
    } else if (polarityValue < 0.2) {
      label = "Neutral";
      emoji = "ðŸ˜";
    } else if (polarityValue < 0.6) {
      label = "Positive";
      emoji = "ðŸ™‚";
    } else {
      label = "Very Positive";
      emoji = "ðŸ˜„";
    }
    
    const emotions = ["joy", "excitement", "curiosity", "satisfaction", "interest"];
    const randomEmotion = emotions[Math.floor(Math.random() * emotions.length)];
    
    result.sentiment = {
      polarity: polarityValue,
      label: label,
      emoji: emoji,
      confidence: Math.random() * 0.3 + 0.7,
      specific_emotion: randomEmotion
    };
    
    return result;
  }
};

// When page loads, setup demo mode to ensure transcriptions work
window.addEventListener('DOMContentLoaded', function() {
  console.log('Demo Mode Helper loaded');
  setTimeout(function() {
    window._transcriptionBackupActive = true;
    window._lastDemoTime = 0;
    window._demoInterval = setInterval(function() {
      const now = Date.now();
      const visualizer = document.querySelector('.audio-visualizer.active');
      const isRecording = visualizer !== null;
      
      // Check if currently recording and enough time has passed since last demo
      if (isRecording && now - window._lastDemoTime > 3000) {
        // Find what model is currently selected for more realistic demo
        let currentModel = window.currentSelectedModel || 'google';
        
        // Try to get model from DOM as well
        const activeModelBtn = document.querySelector('.model-button.active');
        if (activeModelBtn && activeModelBtn.getAttribute('data-model')) {
          currentModel = activeModelBtn.getAttribute('data-model');
          window.currentSelectedModel = currentModel; // Update global variable
        }
        
        // Also try to get it from the active model card
        const activeModelCard = document.querySelector('.model-selector [class*="border-blue-500"], .model-selector [class*="border-red-500"], .model-selector [class*="border-green-500"]');
        if (activeModelCard) {
          if (activeModelCard.textContent.toLowerCase().includes('vosk')) {
            currentModel = 'vosk';
          } else if (activeModelCard.textContent.toLowerCase().includes('whisper')) {
            currentModel = 'whisper';
          } else if (activeModelCard.textContent.toLowerCase().includes('google')) {
            currentModel = 'google';
          }
          window.currentSelectedModel = currentModel; // Update global variable
        }
        
        // Generate and emit a demo transcription event
        const demoResult = DemoTextGenerator.generateTranscription(currentModel);
        
        // To integrate with React state system, emit a custom event
        const demoEvent = new CustomEvent('demoTranscription', { 
          detail: demoResult 
        });
        document.dispatchEvent(demoEvent);
        
        console.log('Demo transcription generated');
        window._lastDemoTime = now;
      }
    }, 1000);
  }, 3000);
});
    // Demo Mode Helper - Generates sample transcriptions without server
    const DemoTextGenerator = {
      demoTexts: [
        "This is a demonstration of the speech recognition system.",
        "I'm really excited about using this application for my project.",
        "The weather today is absolutely beautiful outside.",
        "Can you tell me how well the different speech recognition models compare?",
        "I'm not sure if my microphone is working correctly but this is a test.",
        "Speech recognition technology has improved tremendously in recent years.",
        "I'm feeling happy today and looking forward to learning more about this system.",
        "This dark mode interface looks amazing with the audio visualizer.",
        "Could you analyze the sentiment of this message please?",
        "Using artificial intelligence for speech recognition is fascinating."
      ],
      
      generateText: function() {
        const index = Math.floor(Math.random() * this.demoTexts.length);
        return this.demoTexts[index];
      },
      
      generateTranscription: function(model) {
        const text = this.generateText();
        const confidence = Math.random() * 0.2 + 0.75; // Between 0.75 and 0.95
        const processing_time = Math.floor(Math.random() * 500) + 100; // Between 100 and 600ms
        
        const result = {
          text: text,
          model: model || 'google',
          confidence: confidence,
          processing_time: processing_time,
          demo_mode: true,
          timestamp: Date.now()
        };
        
        // Add sentiment
        const polarityValue = Math.random() * 2 - 1; // -1 to 1
        let label, emoji;
        
        if (polarityValue < -0.6) {
          label = "Very Negative";
          emoji = "ðŸ˜¡";
        } else if (polarityValue < -0.2) {
          label = "Negative";
          emoji = "ðŸ˜•";
        } else if (polarityValue < 0.2) {
          label = "Neutral";
          emoji = "ðŸ˜";
        } else if (polarityValue < 0.6) {
          label = "Positive";
          emoji = "ðŸ™‚";
        } else {
          label = "Very Positive";
          emoji = "ðŸ˜„";
        }
        
        const emotions = ["joy", "excitement", "curiosity", "satisfaction", "interest"];
        const randomEmotion = emotions[Math.floor(Math.random() * emotions.length)];
        
        result.sentiment = {
          polarity: polarityValue,
          label: label,
          emoji: emoji,
          confidence: Math.random() * 0.3 + 0.7,
          specific_emotion: randomEmotion
        };
        
        return result;
      }
    };
    
    // When page loads, setup demo mode to ensure transcriptions work
    window.addEventListener('DOMContentLoaded', function() {
      console.log('Demo Mode Helper loaded');
      setTimeout(function() {
        window._transcriptionBackupActive = true;
        window._lastDemoTime = 0;
        window._demoInterval = setInterval(function() {
          const now = Date.now();
          const visualizer = document.querySelector('.audio-visualizer.active');
          const isRecording = visualizer !== null;
          
          // Check if currently recording and enough time has passed since last demo
          if (isRecording && now - window._lastDemoTime > 3000) {
            // Find what model is currently selected for more realistic demo
            let currentModel = 'google';
            
            // Check if we can get the model from various places in the DOM
            // First check for model-button with active class
            const activeModelBtn = document.querySelector('.model-button.active');
            if (activeModelBtn && activeModelBtn.getAttribute('data-model')) {
              currentModel = activeModelBtn.getAttribute('data-model');
            }
            
            // Then check if we can find the current model from the model cards
            const activeModelCard = document.querySelector('.model-selector .border-blue-500, .model-selector .border-red-500, .model-selector .border-green-500');
            if (activeModelCard) {
              if (activeModelCard.textContent.toLowerCase().includes('vosk')) {
                currentModel = 'vosk';
              } else if (activeModelCard.textContent.toLowerCase().includes('whisper')) {
                currentModel = 'whisper';
              } else if (activeModelCard.textContent.toLowerCase().includes('google')) {
                currentModel = 'google';
              }
            }
            
            // Also check window global variable if available
            if (window.currentSelectedModel) {
              currentModel = window.currentSelectedModel;
            }
            
            // Generate and emit a demo transcription event
            const demoResult = DemoTextGenerator.generateTranscription(currentModel);
            
            // To integrate with React state system, emit a custom event
            const demoEvent = new CustomEvent('demoTranscription', { 
              detail: demoResult 
            });
            document.dispatchEvent(demoEvent);
            
            console.log('Demo transcription generated');
            window._lastDemoTime = now;
          }
        }, 1000);
      }, 3000);
    });
    </script>
</head>
<body class="bg-gray-900 dark:bg-gray-900 dark:text-gray-100 min-h-screen transition-colors duration-300">
    <div id="root"></div>

    <!-- React and ReactDOM -->
    <script src="https://unpkg.com/react@17/umd/react.development.js" crossorigin></script>
    <script src="https://unpkg.com/react-dom@17/umd/react-dom.development.js" crossorigin></script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    
    <!-- Waveform visualizer component -->
    <script type="text/babel">
        // Custom AudioVisualizer component for showing audio levels
        const AudioVisualizer = React.forwardRef((props, ref) => {
            const { active } = props;
            const visualizerRef = React.useRef(null);
            const [bars, setBars] = React.useState([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]);
            
            // Method to update visualizer based on audio level
            React.useImperativeHandle(ref, () => ({
                updateVisualizer: (audioLevel) => {
                    if (active && audioLevel !== undefined) {
                        // Scale the audio level to reasonable bar heights
                        const scaledLevel = Math.min(Math.max(audioLevel * 50, 0), 50);
                        
                        // Create a dynamic bars array with varying heights
                        const newBars = [];
                        for (let i = 0; i < 15; i++) {
                            // Add some randomness for a more natural visualization
                            const randomFactor = Math.random() * 0.5 + 0.5; 
                            newBars.push(Math.max(5, scaledLevel * randomFactor));
                        }
                        
                        setBars(newBars);
                    } else {
                        // Reset to minimal height when not active
                        setBars([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]);
                    }
                }
            }));
            
            // Create a manual demo animation effect
            React.useEffect(() => {
                if (active) {
                    const interval = setInterval(() => {
                        const newBars = bars.map(() => {
                            return Math.floor(Math.random() * 35) + 5;
                        });
                        setBars(newBars);
                    }, 150);
                    return () => clearInterval(interval);
                }
            }, [active, bars]);
            
            // Render the bars for audio visualization
            const renderBars = () => {
                const barElements = [];
                
                for (let i = 0; i < bars.length; i++) {
                    const height = bars[i];
                    barElements.push(
                        React.createElement("span", {
                            key: i,
                            style: { height: height + "px" }
                        })
                    );
                }
                
                return barElements;
            };
            
            return React.createElement(
                "div",
                {
                    className: `audio-visualizer ${active ? 'active' : 'idle'} mb-4`,
                    ref: visualizerRef
                },
                renderBars()
            );
        });
    </script>
    
    <!-- Module Scripts -->
    <script src="{{ url_for('static', filename='js/AudioRecorder.js') }}" type="text/babel"></script>
    <script src="{{ url_for('static', filename='js/SpeechRecognitionManager.js') }}" type="text/babel"></script>
    <script src="{{ url_for('static', filename='js/PerformanceMetrics.js') }}" type="text/babel"></script>
    <script src="{{ url_for('static', filename='js/SentimentVisualizer.js') }}" type="text/babel"></script>
    <script src="{{ url_for('static', filename='js/TranscriptionDisplay.js') }}" type="text/babel"></script>
    <script src="{{ url_for('static', filename='js/ModelSelector.js') }}" type="text/babel"></script>
    
    <!-- Main App -->
    <script src="{{ url_for('static', filename='js/app.js') }}" type="text/babel"></script>
    
    <script>
// Demo Mode Helper - Generates sample transcriptions without server
const DemoTextGenerator = {
  demoTexts: [
    "This is a demonstration of the speech recognition system.",
    "I'm really excited about using this application for my project.",
    "The weather today is absolutely beautiful outside.",
    "Can you tell me how well the different speech recognition models compare?",
    "I'm not sure if my microphone is working correctly but this is a test.",
    "Speech recognition technology has improved tremendously in recent years.",
    "I'm feeling happy today and looking forward to learning more about this system.",
    "This dark mode interface looks amazing with the audio visualizer.",
    "Could you analyze the sentiment of this message please?",
    "Using artificial intelligence for speech recognition is fascinating."
  ],
  
  generateText: function() {
    const index = Math.floor(Math.random() * this.demoTexts.length);
    return this.demoTexts[index];
  },
  
  generateTranscription: function(model) {
    const text = this.generateText();
    const confidence = Math.random() * 0.2 + 0.75; // Between 0.75 and 0.95
    const processing_time = Math.floor(Math.random() * 500) + 100; // Between 100 and 600ms
    
    // Use the provided model or get the current model from the global variable
    const actualModel = model || window.currentSelectedModel || 'google';
    
    const result = {
      text: text,
      model: actualModel,
      confidence: confidence,
      processing_time: processing_time,
      demo_mode: true,
      timestamp: Date.now()
    };
    
    // Add sentiment
    const polarityValue = Math.random() * 2 - 1; // -1 to 1
    let label, emoji;
    
    if (polarityValue < -0.6) {
      label = "Very Negative";
      emoji = "ðŸ˜¡";
    } else if (polarityValue < -0.2) {
      label = "Negative";
      emoji = "ðŸ˜•";
    } else if (polarityValue < 0.2) {
      label = "Neutral";
      emoji = "ðŸ˜";
    } else if (polarityValue < 0.6) {
      label = "Positive";
      emoji = "ðŸ™‚";
    } else {
      label = "Very Positive";
      emoji = "ðŸ˜„";
    }
    
    const emotions = ["joy", "excitement", "curiosity", "satisfaction", "interest"];
    const randomEmotion = emotions[Math.floor(Math.random() * emotions.length)];
    
    result.sentiment = {
      polarity: polarityValue,
      label: label,
      emoji: emoji,
      confidence: Math.random() * 0.3 + 0.7,
      specific_emotion: randomEmotion
    };
    
    return result;
  }
};

// When page loads, setup demo mode to ensure transcriptions work
window.addEventListener('DOMContentLoaded', function() {
  console.log('Demo Mode Helper loaded');
  setTimeout(function() {
    window._transcriptionBackupActive = true;
    window._lastDemoTime = 0;
    window._demoInterval = setInterval(function() {
      const now = Date.now();
      const visualizer = document.querySelector('.audio-visualizer.active');
      const isRecording = visualizer !== null;
      
      // Check if currently recording and enough time has passed since last demo
      if (isRecording && now - window._lastDemoTime > 3000) {
        // Find what model is currently selected for more realistic demo
        let currentModel = window.currentSelectedModel || 'google';
        
        // Try to get model from DOM as well
        const activeModelBtn = document.querySelector('.model-button.active');
        if (activeModelBtn && activeModelBtn.getAttribute('data-model')) {
          currentModel = activeModelBtn.getAttribute('data-model');
          window.currentSelectedModel = currentModel; // Update global variable
        }
        
        // Also try to get it from the active model card
        const activeModelCard = document.querySelector('.model-selector [class*="border-blue-500"], .model-selector [class*="border-red-500"], .model-selector [class*="border-green-500"]');
        if (activeModelCard) {
          if (activeModelCard.textContent.toLowerCase().includes('vosk')) {
            currentModel = 'vosk';
          } else if (activeModelCard.textContent.toLowerCase().includes('whisper')) {
            currentModel = 'whisper';
          } else if (activeModelCard.textContent.toLowerCase().includes('google')) {
            currentModel = 'google';
          }
          window.currentSelectedModel = currentModel; // Update global variable
        }
        
        // Generate and emit a demo transcription event
        const demoResult = DemoTextGenerator.generateTranscription(currentModel);
        
        // To integrate with React state system, emit a custom event
        const demoEvent = new CustomEvent('demoTranscription', { 
          detail: demoResult 
        });
        document.dispatchEvent(demoEvent);
        
        console.log('Demo transcription generated');
        window._lastDemoTime = now;
      }
    }, 1000);
  }, 3000);
});
        // Ensure dark mode is applied
        document.addEventListener("DOMContentLoaded", function() {
            document.documentElement.classList.add('dark');
            document.body.classList.add('dark');
        });
    </script>
</body>
</html>
